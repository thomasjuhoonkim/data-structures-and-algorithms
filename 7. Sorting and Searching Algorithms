Bubble Sort:
    - Bubble sort is a sorting algorithm that compares two adjacent elements and swaps them until
      they are not in the intended order.
    - Just like the movement of air bubbles in the ater that rise up to the surface, each element
      of the array moves to the end in each iteration. Therefore, it is called a bubble sort.
    
    Working of Bubble Sort:
        - Suppose we are trying to sort the elements in ascending order.
        1. First Iteration (Compare and Swap)
            1. Starting from the first index, compare the first and the second elements.
            2. If the first element is greater than the second element, they are swapped.
            3. Now, compare the second and the third elements. Swap them if they are not in order.
            4. The above process goes on until the last element.
        2. Remaining Iteration
            - The same process goes on for the remaining iterations.
            - After each iteration, the largest element among the unsorted elements is placed
              at the end.
            - In each iteration, the comparison takes place up to the last unsorted element.
            - The array is sorted when all the unsorted elements are placed at their
              correct positions.
    
    Bubble Sort Algorithm:
        bubbleSort(array)
            for i <- 1 to indexOfLastUnsortedElement-1
                if leftElement > rightElement
                    swap leftElement and rightElement
        end bubbleSort
    
    Bubble Sort Code: Example Files

    Optimized Bubble Sort Algorithm:
        - In the above algorithm, all the comparions are made even if the array is already sorted.
        - This increases the execution time
        - To solve this, we can introduce an extra variable swapped. The value of swapped is set
          true if there occurs swapping of elements. Otherwise, it is set to false.
        - After an iteration, if there is no swapping, the value of swapped will be false.
          This means elements are already sorted and there is no need to perform further iterations.
        - This will reduce the execution time and helpes to optimize the bubble sort.

        Algorithm for Optimized Bubble Sort:
            bubbleSort(array)
                swapped <- false
                for i <- 1 to indexOfLastUnsortedElement-1
                    if leftElement > rightElement
                        swap leftElement and rightElement
                        swapped <- true
            end bubbleSort
    
    Bubble Sort Complexity:
        Time Complexity:
            Best: O(n)
            Worst: O(n^2)
            Average O(n^2)
        Space Complexity: O(1)
        Stability: Yes
    
    Complexity in Detail:
        Cycle   Number of Copmarisons
        1st     (n-1)
        2nd     (n-2)
        3rd     (n-3)
        ...     .....
        last    1

        The number of comparisons is (n-1) + (n-2) + (n-3) +...+ 1 = n(n-1)/2
        nearly equals to n^2

        - Hence Complexity O(n^2)
        - Also, if we observe the code, bubble sort requires two loops. Hence the complexity
          n*n = n^2
        
        Time Complexities:
            Worst Case Complexity: O(n^2)
                - If we want to sort in ascending order and the array is in descending order then
                  the worst case occurs.
            Best Case Complexity: O(n)
                - If the array is already sorted, then there is no need for sorting.
            Average Case Complexity: O(n^2)
                - It occurs when the elements of the array are in jumbled order (neither ascending
                  nor descending).
        Space Complexity:
            - Space complexity is O(1) because an extra vaariable is used for swapping.
            - In the optimized bubble sort algorithm, two extra variables are used. Hence, the
              space complexity will be O(2).
        
    Bubble Sort Applications:
        - Bubble sort is used if
            - complexity does not matter
            - short and simple code is preferred
    
    Similar Sorting Algorithms
        - Quicksort
        - Insertion sort
        - Merge sort
        - Selection sort

====================================================================================================

Selection Sort:
    - Selection sort is a sorting algorithm that selects the smallest element from an unsorted list
      in each iteration and places that element at the beginning of the unsorted list.
    
    Working of Selection Sort:
        1. Set the first element as minimum.
        2. Compare minimum with the second element. If the second element is smaller than minimum,
           assign the second element as minimum.

           Compare minimum with the third element. Again, if the third element is smaller, then
           assign minimum to the third element otherwise do nothing. The process goes on until
           the last element.
        3. After each iteration, minimum is placed in the front of the unsorted list.
        4. For each iteration, indexing starts from the first unsorted element. Step 1 to 3 are
           repeated until all the elements are placed at their correct positions.
    
    Selection Sort Algorithm:
        selectionSort(array, size)
            repeat (size - 1) times
            set the first unsorted element as the minimum
            for each of the unsorted elements
                if element < currentMinimum
                    set element as new minimum
            swap minimum with first unsorted position
        end selectionSort
    
    Selection Sort Code: Example Files

    Selection Sort Complexity:
        Time Complexity:
            Best: O(n^2)
            Worst: O(n^2)
            Average: O(n^2)
        Space Complexity: O(1)
        Stability: No

        Cycle   Number of Comparisons 
        1st     (n-1)
        2nd     (n-2)
        3rd     (n-3)
        ...     .....
        last    1

        - Number of comparisons: (n-1) + (n-2) + (n-3) + ... + 1 = n(n-1)/2 nearly equals to n^2
        - Complexity = O(n^2)
        - Also, we can analyse the complexity by simply observing the number of loops.
          There are 2 loops so the complexity is n*n = n^2.
        
        Time Complexities:
            Worst Case Complexity: O(n^2)
                - If we want to sort in ascending order and the array is in descending order then,
                  the worst case occurs.
            Best Case Complexity: O(n^2)
                - It occurs when the array is already sorted
            Average Case Complexity: O(n^2)
                - It occurs when the elements of the array are in jumbled order (neither ascending
                  nor descending)
            
            The time complexity of the selection sort is the same in all cases. At every step,
            you have to find the minimum element and put it in the right place. The minimum element
            is not known until the end of the array is not reached.
        
        Space Complexity:
            - Space complexity is O(1) because an extra variable temp is used.
    
    Selection Sort Applications:
        The selection sort is used when:
            - a small list is to be sorted.
            - cost of swapping does not matter
            - checking if all the elements is compulsory
            - cost of writing to a memory matters like in flash memory (number of writes/swaps is
              O(n) as compared to O(n^2) of bubble sort)
    
    Similar Sorting Algorithms:
        - Bubble sort
        - Quicksort
        - Insertion sort
        - Merge sort

=================================================================================================

Insertion Sort Algorithm:
    - Insertion sort is a sorting algorithm that palces an unsorted element at its suitable place
      in each iteration.
    - Insertion sort works similarly as we sort cards in our hand in a card game.
    - We assume that the first card is already sorted then, we select an unsorted card. If the
      unsorted card is greater than the card in hand, it is placed on the right otherwise, to
      the left. In the same way, other unsorted cards are taken and put in the right place.
    - A similar approach is used by insertion sort.

    Working of Insertion Sort:
        1. The first element in the array is assumed to be sorted. Take the second element and
           store it separately in key.

           Compare key with the first element. If the first element is greater than key, then key is
           placed in front of the first element.
        
        2. Now, the first two elements are sorted.

           Take the third element and compare it with the elements on the left of it. Place it just
           behind the element smaller than it. If there is no element smaller than it, then place it
           at the beginning of the array.

        3. Similarly, place every unsorted element at its correct position.
    
    Insertion Sort Algorithm:
        insertionSort(array)
            mark first element as sorted
            for each unsorted element X
                'extract' the element X
                for j <- lastSortedIndex down to 0
                    if current element j > X
                        move sorted element to the right by 1
                break loop and insert X here
        end insertionSort
    
    Insertion Sort Code: Example Files

    Insertion Sort Complexity:
        Time Complexity:
            Best: O(n)
            Worst: O(n^2)
            Average: O(n^2)
        Space Complexity: O(1)
        Stability: Yes

        Time Complexities:
            Worst Case Complexity: O(n^2)
                - Suppose, an array is in ascending order, and you want to sort it in
                  descending order. In this case, worst case complexity occurs.
                - Each element has to be compared with each of the other elements so, for every
                  nth element, (n-1) number of comparisons are made.
                - Thus, the total number of comparisons = n*(n-1) ~ n^2
            Best Case Complexity: O(n)
                - When the array is already sorted, the outer loop runs for n number of times whereas
                  the inner loop does not run at all. So, there are only n number of comparison.
                - Thus, complexity is linear.
            Average Case Complexity:
                - It occurs when the elements of an array are in jumbled order (neither ascending
                  nor descending)
        
        Space Complexity:
            - Space complexity is O(1) because an extra variable key is used.
    
    Insertion Sort Applications:
        The insertion sort is used when:
            - the array has a small number of elements.
            - there are only a few elements left to be sorted
    
    Similar Sorting Algorithms:
        1. Bubble Sort
        2. Quicksort
        3. Merge Sort
        4. Selection Sort

====================================================================================================

Merge Sort Algorithm:
    - Merge sort is one of the most popular sorting algorithms that is based on the principle of
      Divide and Conquer Algorithm.
    - Here, a problem is divided into multiple sub-problems.
    - Each sub-problem is solved individually. Finally, sub-problems are combined to form the
      final solution.
    
    Divide and Conquer Strategy:
        - Using the Divide and Conquer technique, we divide a problem into subproblems.
          When the solution to each subproblem is ready, we 'combine' the results from the subproblems
          to solve the main problem.
        - Suppose we had to sort an array A. A subproblem would be to sort a sub-section of this array
          starting at index p and ending at index r, denoted as A[p..r].
        
        Divide:
            - If q is the half-way point between p and r, then we can split the subarray A[p..r] into
              two arrays A[p..q] and A[q+1, r].
        
        Conquer:
            - In the conquer step, we try to sort both the subarrays A[p..q] and A[q+1, r].
              If we haven't yet reached the base case, we again divide both these subarrays
              and try to sort them.
        
        Combine:
            - When the conquer step reaches the base step and we get two sorted subarrays A[p..q] and A[q+1, r]
              for array A[p..r], we combine the results by creating a sorted array A[p..r] from two sorted subarrays
              A[p..q] and A[q+1, r].
        
    Merge Sort Algorithm:
        - The merge sort function repeatdly divides the array into two halves until we reach a stage where we try
          to perform merge sort on a subarray of size 1 ie. p == r.
        - After that, the merge function comes into play and combines the sorted arrays into larger arrays until
          until the whole array is merged.
        
        mergeSort(A, p, r):
            if p > r 
                return
            q = (p+r)/2
            mergeSort(A, p, q)
            mergeSort(A, q+1, r)
            merge(A, p, q, r)
        
        - To sort an entire array, we need to call mergeSort(A, 0, length(A)-1).
        - As shown in the image below, the merge sort algorithm recursively divides the array into havles until
          we reach the base case of array with 1 element. After that, the merge function picks up the sorted sub-arrays
          and merges them to gradually sort the entire array.
        
        The merge Step of Merge Sort:
            - Every recursive algorhtm is dependent on a base case and the ability to combine the results from base cases.
              Merge sort is no different. The most oimportant part of the merge sort algorithm is, you guessed it, merge step.
            - The merge step is the solution to the simple problem of merging two sorted lists(arrays) to build one large sorted list(array).
            - The algorhtm maintains three pointers, one for each of the two arrays and one for maintaining the current index of the final
              sorted array.
            
            Have we reached the end of any of the arrays?
                No:
                    Compare current elements of both arrays 
                    Copy smaller element into sorted array
                    Move pointer of element containing smaller element
                Yes:
                    Copy all remaining elements of non-empty array
        
        Writing the Code for Merge Algorithm:
            - A noticeable difference between the merging step we described above and the one we use for merge sort is that we only perform
              the merge function on consecutive sub-arrays.
            - This is why we only need the array, the first position, the last index of the first subarray(we can calculate the first index of the second subarray)
              and the last index of the second subarray.
            - Our task is to merge two subarrays A[p..q] and A[q+1..r] to create a sorted array A[p..r]. So the inputs to the function are A,p,q, and r
            The merge function works as follows:
                1. Create copies of the subarrays L <- A[p..q] and M <- A[q+1..r].
                2. Create three pointers i, j, and k
                    a. i maintains current index of L, starting at 1
                    b. j maintains current index of M, starting at 1
                    c. k maintains the current index of A[p..q], starting at p.
                3. Until we reach the end of either L or M, pick the larger among the elements from L and M and place them in the correct position at A[p..q]
                4. WHen we run out of elements in either L or M, pick up the remaining elements and put in A[p..q].
            
            In code:
                // Merge two subarrays L and M into arr
                void merge(int arr[], int p, int q, int r) {

                    // Create L ← A[p..q] and M ← A[q+1..r]
                    int n1 = q - p + 1;
                    int n2 = r - q;

                    int L[n1], M[n2];

                    for (int i = 0; i < n1; i++)
                        L[i] = arr[p + i];
                    for (int j = 0; j < n2; j++)
                        M[j] = arr[q + 1 + j];

                    // Maintain current index of sub-arrays and main array
                    int i, j, k;
                    i = 0;
                    j = 0;
                    k = p;

                    // Until we reach either end of either L or M, pick larger among
                    // elements L and M and place them in the correct position at A[p..r]
                    while (i < n1 && j < n2) {
                        if (L[i] <= M[j]) {
                            arr[k] = L[i];
                            i++;
                        } else {
                            arr[k] = M[j];
                            j++;
                        }
                        k++;
                    }

                    // When we run out of elements in either L or M,
                    // pick up the remaining elements and put in A[p..r]
                    while (i < n1) {
                        arr[k] = L[i];
                        i++;
                        k++;
                    }

                    while (j < n2) {
                        arr[k] = M[j];
                        j++;
                        k++;
                    }
                }
        
        Merge() Function Explained Step-By-Step
            - A lot is happening in this function, so let's take an example to see how this would work.
            - The array A[0..5] contains two sorted subarrays A[0..3] and A[4..5]. Let us see how the merge
              function will merge the two arrays.
                void merge(int arr[], int p, int q, int r) {
                // Here, p = 0, q = 4, r = 6 (size of array)
            Step 1: Create duplicate copies of sub-arrays to be sorted:
                // Create L ← A[p..q] and M ← A[q+1..r]
                int n1 = q - p + 1 = 3 - 0 + 1 = 4;
                int n2 = r - q = 5 - 3 = 2;

                int L[4], M[2];

                for (int i = 0; i < 4; i++)
                    L[i] = arr[p + i];
                    // L[0,1,2,3] = A[0,1,2,3] = [1,5,10,12]

                for (int j = 0; j < 2; j++)
                    M[j] = arr[q + 1 + j];
                    // M[0,1] = A[4,5] = [6,9]
            Step 2: Maintain current index of sub-arrays and main array
                int i, j, k;
                i = 0; 
                j = 0; 
                k = p;
            Step 3: Until we reach the end of either L or M, pick larger among elements L and M and place
                    them in the correct position at A[p..r]
                while (i < n1 && j < n2) { 
                    if (L[i] <= M[j]) { 
                        arr[k] = L[i]; i++; 
                    } 
                    else { 
                        arr[k] = M[j]; 
                        j++; 
                    } 
                    k++; 
                }
            Step 4: When we run out of elements in either L or M, pick up the remaining elements
                    and put in A[p..r]
                // We exited the earlier loop because j < n2 doesn't hold
                while (i < n1)
                {
                    arr[k] = L[i];
                    i++;
                    k++;
                }
                // We exited the earlier loop because i < n1 doesn't hold  
                while (j < n2)
                {
                    arr[k] = M[j];
                    j++;
                    k++;
                }
            }
                - This step would have been needed if the size of M was greater than L.
                - At the end of the merge function, the subarray A[p..r] is sorted.

    Merge Sort Complexity:
        Time Complexity:
            Best: O(n*log n)
            Worst: O(n*log n)
            Average: O(n*log n)
        Space Complexity: O(n)
        Stability: Yes
    
    Merge Sort Applications:
        - Inversion count problem
        - External sorting
        - E-commerce applications
    
    Similar Sorting Algorithms:
        - Quicksort
        - Insertion sort
        - Selection sort
        - Bucket sort
    
=============================================================================================

Quicksort Algorithm:
    - Quicksort is a sorting algorithm based on the divide and conquer approach where
        1. An array is divided into subarrays by selecting a pivot element (element selected from the array).

           While dividing the array, the pivot element should be positioned in such a way that elements
           less than pivot are kept on the left side and elements are greater than pivot are on the
           right side of the pivot.
        
        2. The left and right subarrays are also divided using the same approach. This process
           continues until each subarray contains a single element.
        
        3. At this point, elements are already sorted. Finally, elements are combined to form a sorted array.
    
    Working of Quicksort Algorith:
        1. Select the Pivot element:
            - There are different variations of quicksort where the pivot element is selected from different positions.
              Here, we will be selecting the rightmost element of the array as the pivot element.
        2. Rearrange the array:
            - Now the elements of the array are rearranged so that elements that are smaller than the pivot are put
              on the left and the elements greater than the pivot are put on the right.

              Here's how we rearrange the array:
                1. A pointer is fixed at the pivot element. The pivot element is compared with the elements
                   beginning from the first index.
                2. If the element is greater than the pivot element, a second pointer is set for that element.
                3. Now, pivot is compared with other elements. If an element smaller than the pivot element is reached,
                   the smaller element is swapped with the greater element found earlier.
                4. Again, the process is repeated to set the next greater element as the second pointer. And, swap it with
                   another smaller element.
                5. The process goes on until the second last element is reached.
                6. Finally, the pivot element is swapped with the second pointer.
        3. Divide Subarrays:
            - Pivot elements are again chosen for the left and the right sub-parts separately. And, step 2 is repeated.
            - The subarrays are divided until each subarray is formed of a single element. At this point, the array is already sorted.
    
    Quicksort Algorithm:
        quickSort(array, leftmostIndex, rightmostIndex)
            if (leftmostIndex < rightmostIndex)
                pivotIndex <- partition(array,leftmostIndex, rightmostIndex)
                quickSort(array, leftmostIndex, pivotIndex - 1)
                quickSort(array, pivotIndex, rightmostIndex)

        partition(array, leftmostIndex, rightmostIndex)
            set rightmostIndex as pivotIndex
            storeIndex <- leftmostIndex - 1
            for i <- leftmostIndex + 1 to rightmostIndex
                if element[i] < pivotElement
                    swap element[i] and element[storeIndex]
                    storeIndex++
            swap pivotElement and element[storeIndex+1]
            return storeIndex + 1
    
    Quicksort Complexity:
        Time Complexity:
            Best: O(n*log n)
            Worst: O(n^2)
            Average: O(n*log n)
        Space Complexity: O(log n)
        Stability: No
    
    Time Complexities:
        Worst Case Complexity [Big-O]: O(n^2):
            - It occurs when the pivot element picked is either the greatst or the smallest element.
            - This condition leads to the case in which the pivot element lies in an extreme end of the sorted array.
              One sub-array os always empty and another sub-array contains n-1 element. Thus, quicksort
              is called only on this sub-array.
            - However, the quicksort algorithm has better performance for scattered pivots.
        Best Case Complexity [Big-Omega]: O(n*log n):
            - It occurs when the pivot elemnt is always the middle element or near to the middle element.
        Average Case Complexity [Big-Theta]: O(n*log n):
            - It occurs when the above conditions do not occur.
    
    Space Complexity:
        - The space complexity for quicksort is O(log n)
    
    Quicksort Applications:
        - The programming language is good for recursion
        - Time complexity matters
        - Space complexity matters
    
    Simiar Sorting Algorithms:
        - Insertion Sort
        - Merge Sort
        - Selection Sort
        - Bucket Sort

=============================================================================================================================

Counting Sort Algorithm:
    - Counting sort is a sorting algorithm that sorts the elements of an array by counting the number of occurrences of each
      unique element in the array.
    - The count is stored in an auxiliary array and the sorting is done by mapping the count as an index of the auxiliary array.

    Working of Counting Sort:
        1. Find out hte maximum element (let it be max) from the given array.
        2. Initialize an array of length max+1 with all elements 0.
           This array is used for storing the count of the elements in the array.
        3. Store the count of each element at their respective index in count array
           For example: if the count of element 3 is 2 then, 2 is stored in the 3rd position of count array. If element "5"
           is not present in the array, then 0 is stored in 5th position.
        4. Store cumulative sum of the elements of the count array. It helps in placing the elements into the correct index
           of the stored array.
        5. Find the index of each element of the original array in the count array. This gives the cumulative count.
           Place the element at the index calculated as shown in figure below.
        6. After placing each element at its correct position, decrease its count by one.
    
    Counting Sort Algorithm:
        countingSort(array, size)
            max <- find largest element in array
            initialize count array with all zeros
            for j <- 0 to size
                find the total count of each unique element and 
                store the count at jth index in count array
            for i <- 1 to max
                find the cumulative sum and store it in count array itself
            for j <- size down to 1
                restore the elements to array
                decrease count of each element restored by 1
    
    Counting Sort Code: exmaple files

    Complexity:
        Time Complexity:
            Best: O(n+k)
            Worst: O(n+k)
            Average: O(n+k)
        Space Complexity: O(max)
        Stability: Yes
    
    Time Complexities:
        There are mainly four main loops. (Finding the greatest
        value can be done outside the function)
        for-loop    time of counting
        1st         O(max)
        2nd         O(size)
        3rd         O(max)
        4th         O(size)
        - Overall copmlexity = O(max)+O(size)+O(max)+O(size) = O(max+size)
        - Worst Case Complexity: O(n+k)
        - Best Case Complexity: O(n+k)
        - Average Case Complexity: O(n+k)
        - In all the above cases, the complexity is the same because 
          no matter how the elements are placed in the array, the algorithm goes through n+k times.
        - There is no comparison between any elements, so it is bad if the integers are very large
          because the array of that size should be made.
    
    Space Complexity:
        - The space complexity of Counting Sort is O(max). Larger the range of elements, larger the space complexity.

    Counting Sort Applications:
        - there are smaller integers with multiple counts.
        - linear complexity is the need.
    
    Similar Sorting Algorthims:
        - Quicksort
        - Merge Sort
        - Bucket Sort
        - Radix Sort

=======================================================================================================================

Radix Sort Algorithm:
    - Radix sort is a sorting algorithm that sorts the elements by first grouping the individual digits of the same place value.
    - Then, sort the elements according to their increasing/decreasing order.
    - Suppose, we have an array of 8 elements. First, we will sort elements based on the value of the unit place.
    - Then, we will sort elements based on the value of the tength place.
    - This process goes on until the last significant place.
    - Let the initial array be [121, 432, 562, 23, 1, 45, 788]. It is sorted according to radix sort.

    Working of Radix Sort:
        1. Find the largest element in the array, ie. max. Let X be the number of digits in max. X is calculated because
           we have to go through all the significant places of all elements.
           In this array [121, 432, 564, 23, 1, 45, 788], we have the largest number 788. It has 3 digits. Therefore,
           the loop should go up to hundreds place (3 times).
        2. Now go through each signficiant place one by one.
           Use any stable sorting technique to sort the digits at each significant place. We have used counting sort.
           Sort the elements based on the unit place digits (X=0).
        3. Now, sort the elements based on digits at tense place.
        4. Finally, sort the elements based on the digits at hundreds place.
    
    Radix Sort Algorithm:
        radixSort(array)
            d <- maximum number of digits in the largest element
            create d buckets of size 0-9
            for i <- 0 to d
                sort the elements according to ith place digits using countingSort

        countingSort(array, d)
            max <- find largest element among dth place elements
            initialize count array with all zeros
            for j <- 0 to size
                find the total count of each unique digit in dth place of elements and
                store the count at jth index in count array
            for i <- 1 to max
                find the cumulative sum and store it in count array itself
            for j <- size down to 1
                restore the elements to array
                decrease count of each element restored by 1
        
    Radix Sort Code: example files

    Radix Sort Complexity:
        Time Complexity:
            Best: O(n+k)
            Worst: O(n+k)
            Average: O(n+k)
        Space Complexity: O(max)
        Stability: Yes

        - Since radix sort is a non-comparative algorithm, it has advantages over comparative sorting algorithms.
        - For the radix sort that uses counting sort as an intermediate stable sort, the time complexity is O(d(n+k)).
        - Here, d is the number cycle and O(n+k) is the time complexity of counting sort.
        - Thus, radix sort has linear time complexity which is better than O(nlog n) of comparative sorting algorithms.
        - If we take very large digit numbers or the number of other bases like 32-bit and 64-bit numbers then it can perform
            linear time howeever, the intermediate sort takes large space.
        - This makes radix sort space inefficient. This is the reason why this sort is not used in software libraries.
    
    Radix Sort Applications:
        - DC3 algorithm (Karkkainen-Sanders-Burkhardt) while making a suffix array.
        - places where there are numbers in large ranges.
    
    Similar Sorting Algorithms:
        - Quicksort
        - Merge Sort
        - Bucket Sort
        - Counting Sort
    
============================================================================================================================================

