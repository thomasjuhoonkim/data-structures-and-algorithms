Bubble Sort:
    - Bubble sort is a sorting algorithm that compares two adjacent elements and swaps them until
      they are not in the intended order.
    - Just like the movement of air bubbles in the ater that rise up to the surface, each element
      of the array moves to the end in each iteration. Therefore, it is called a bubble sort.
    
    Working of Bubble Sort:
        - Suppose we are trying to sort the elements in ascending order.
        1. First Iteration (Compare and Swap)
            1. Starting from the first index, compare the first and the second elements.
            2. If the first element is greater than the second element, they are swapped.
            3. Now, compare the second and the third elements. Swap them if they are not in order.
            4. The above process goes on until the last element.
        2. Remaining Iteration
            - The same process goes on for the remaining iterations.
            - After each iteration, the largest element among the unsorted elements is placed
              at the end.
            - In each iteration, the comparison takes place up to the last unsorted element.
            - The array is sorted when all the unsorted elements are placed at their
              correct positions.
    
    Bubble Sort Algorithm:
        bubbleSort(array)
            for i <- 1 to indexOfLastUnsortedElement-1
                if leftElement > rightElement
                    swap leftElement and rightElement
        end bubbleSort
    
    Bubble Sort Code: Example Files

    Optimized Bubble Sort Algorithm:
        - In the above algorithm, all the comparions are made even if the array is already sorted.
        - This increases the execution time
        - To solve this, we can introduce an extra variable swapped. The value of swapped is set
          true if there occurs swapping of elements. Otherwise, it is set to false.
        - After an iteration, if there is no swapping, the value of swapped will be false.
          This means elements are already sorted and there is no need to perform further iterations.
        - This will reduce the execution time and helpes to optimize the bubble sort.

        Algorithm for Optimized Bubble Sort:
            bubbleSort(array)
                swapped <- false
                for i <- 1 to indexOfLastUnsortedElement-1
                    if leftElement > rightElement
                        swap leftElement and rightElement
                        swapped <- true
            end bubbleSort
    
    Bubble Sort Complexity:
        Time Complexity:
            Best: O(n)
            Worst: O(n^2)
            Average O(n^2)
        Space Complexity: O(1)
        Stability: Yes
    
    Complexity in Detail:
        Cycle   Number of Copmarisons
        1st     (n-1)
        2nd     (n-2)
        3rd     (n-3)
        ...     .....
        last    1

        The number of comparisons is (n-1) + (n-2) + (n-3) +...+ 1 = n(n-1)/2
        nearly equals to n^2

        - Hence Complexity O(n^2)
        - Also, if we observe the code, bubble sort requires two loops. Hence the complexity
          n*n = n^2
        
        Time Complexities:
            Worst Case Complexity: O(n^2)
                - If we want to sort in ascending order and the array is in descending order then
                  the worst case occurs.
            Best Case Complexity: O(n)
                - If the array is already sorted, then there is no need for sorting.
            Average Case Complexity: O(n^2)
                - It occurs when the elements of the array are in jumbled order (neither ascending
                  nor descending).
        Space Complexity:
            - Space complexity is O(1) because an extra vaariable is used for swapping.
            - In the optimized bubble sort algorithm, two extra variables are used. Hence, the
              space complexity will be O(2).
        
    Bubble Sort Applications:
        - Bubble sort is used if
            - complexity does not matter
            - short and simple code is preferred
    
    Similar Sorting Algorithms
        - Quicksort
        - Insertion sort
        - Merge sort
        - Selection sort

====================================================================================================

Selection Sort:
    - Selection sort is a sorting algorithm that selects the smallest element from an unsorted list
      in each iteration and places that element at the beginning of the unsorted list.
    
    Working of Selection Sort:
        1. Set the first element as minimum.
        2. Compare minimum with the second element. If the second element is smaller than minimum,
           assign the second element as minimum.

           Compare minimum with the third element. Again, if the third element is smaller, then
           assign minimum to the third element otherwise do nothing. The process goes on until
           the last element.
        3. After each iteration, minimum is placed in the front of the unsorted list.
        4. For each iteration, indexing starts from the first unsorted element. Step 1 to 3 are
           repeated until all the elements are placed at their correct positions.
    
    Selection Sort Algorithm:
        selectionSort(array, size)
            repeat (size - 1) times
            set the first unsorted element as the minimum
            for each of the unsorted elements
                if element < currentMinimum
                    set element as new minimum
            swap minimum with first unsorted position
        end selectionSort
    
    Selection Sort Code: Example Files

    Selection Sort Complexity:
        Time Complexity:
            Best: O(n^2)
            Worst: O(n^2)
            Average: O(n^2)
        Space Complexity: O(1)
        Stability: No

        Cycle   Number of Comparisons 
        1st     (n-1)
        2nd     (n-2)
        3rd     (n-3)
        ...     .....
        last    1

        - Number of comparisons: (n-1) + (n-2) + (n-3) + ... + 1 = n(n-1)/2 nearly equals to n^2
        - Complexity = O(n^2)
        - Also, we can analyse the complexity by simply observing the number of loops.
          There are 2 loops so the complexity is n*n = n^2.
        
        Time Complexities:
            Worst Case Complexity: O(n^2)
                - If we want to sort in ascending order and the array is in descending order then,
                  the worst case occurs.
            Best Case Complexity: O(n^2)
                - It occurs when the array is already sorted
            Average Case Complexity: O(n^2)
                - It occurs when the elements of the array are in jumbled order (neither ascending
                  nor descending)
            
            The time complexity of the selection sort is the same in all cases. At every step,
            you have to find the minimum element and put it in the right place. The minimum element
            is not known until the end of the array is not reached.
        
        Space Complexity:
            - Space complexity is O(1) because an extra variable temp is used.
    
    Selection Sort Applications:
        The selection sort is used when:
            - a small list is to be sorted.
            - cost of swapping does not matter
            - checking if all the elements is compulsory
            - cost of writing to a memory matters like in flash memory (number of writes/swaps is
              O(n) as compared to O(n^2) of bubble sort)
    
    Similar Sorting Algorithms:
        - Bubble sort
        - Quicksort
        - Insertion sort
        - Merge sort

=================================================================================================

Insertion Sort Algorithm:
    - Insertion sort is a sorting algorithm that palces an unsorted element at its suitable place
      in each iteration.
    - Insertion sort works similarly as we sort cards in our hand in a card game.
    - We assume that the first card is already sorted then, we select an unsorted card. If the
      unsorted card is greater than the card in hand, it is placed on the right otherwise, to
      the left. In the same way, other unsorted cards are taken and put in the right place.
    - A similar approach is used by insertion sort.

    Working of Insertion Sort:
        1. The first element in the array is assumed to be sorted. Take the second element and
           store it separately in key.

           Compare key with the first element. If the first element is greater than key, then key is
           placed in front of the first element.
        
        2. Now, the first two elements are sorted.

           Take the third element and compare it with the elements on the left of it. Place it just
           behind the element smaller than it. If there is no element smaller than it, then place it
           at the beginning of the array.

        3. Similarly, place every unsorted element at its correct position.
    
    Insertion Sort Algorithm:
        insertionSort(array)
            mark first element as sorted
            for each unsorted element X
                'extract' the element X
                for j <- lastSortedIndex down to 0
                    if current element j > X
                        move sorted element to the right by 1
                break loop and insert X here
        end insertionSort
    
    Insertion Sort Code: Example Files

    Insertion Sort Complexity:
        Time Complexity:
            Best: O(n)
            Worst: O(n^2)
            Average: O(n^2)
        Space Complexity: O(1)
        Stability: Yes

        Time Complexities:
            Worst Case Complexity: O(n^2)
                - Suppose, an array is in ascending order, and you want to sort it in
                  descending order. In this case, worst case complexity occurs.
                - Each element has to be compared with each of the other elements so, for every
                  nth element, (n-1) number of comparisons are made.
                - Thus, the total number of comparisons = n*(n-1) ~ n^2
            Best Case Complexity: O(n)
                - When the array is already sorted, the outer loop runs for n number of times whereas
                  the inner loop does not run at all. So, there are only n number of comparison.
                - Thus, complexity is linear.
            Average Case Complexity:
                - It occurs when the elements of an array are in jumbled order (neither ascending
                  nor descending)
        
        Space Complexity:
            - Space complexity is O(1) because an extra variable key is used.
    
    Insertion Sort Applications:
        The insertion sort is used when:
            - the array has a small number of elements.
            - there are only a few elements left to be sorted
    
    Similar Sorting Algorithms:
        1. Bubble Sort
        2. Quicksort
        3. Merge Sort
        4. Selection Sort

====================================================================================================

Merge Sort Algorithm:
    - Merge sort is one of the most popular sorting algorithms that is based on the principle of
      Divide and Conquer Algorithm.
    - Here, a problem is divided into multiple sub-problems.
    - Each sub-problem is solved individually. Finally, sub-problems are combined to form the
      final solution.
    
    Divide and Conquer Strategy:
        - Using the Divide and Conquer technique, we divide a problem into subproblems.
          When the solution to each subproblem is ready, we 'combine' the results from the subproblems
          to solve the main problem.
        - Suppose we had to sort an array A. A subproblem would be to sort a sub-section of this array
          starting at index p and ending at index r, denoted as A[p..r].
        
        Divide:
            - If q is the half-way point between p and r, then we can split the subarray A[p..r] into
              two arrays A[p..q] and A[q+1, r].
        
        Conquer:
            - In the conquer step, we try to sort both the subarrays A[p..q] and A[q+1, r].
              If we haven't yet reached the base case, we again divide both these subarrays
              and try to sort them.
        
        Combine:
            - When the conquer step reaches the base step and we get two sorted subarrays A[p..q] and A[q+1, r]
              for array A[p..r], we combine the results by creating a sorted array A[p..r] from two sorted subarrays
              A[p..q] and A[q+1, r].
        
    Merge Sort Algorithm:
        - The merge sort function repeatdly divides the array into two halves until we reach a stage where we try
          to perform merge sort on a subarray of size 1 ie. p == r.
        - After that, the merge function comes into play and combines the sorted arrays into larger arrays until
          until the whole array is merged.
        
        mergeSort(A, p, r):
            if p > r 
                return
            q = (p+r)/2
            mergeSort(A, p, q)
            mergeSort(A, q+1, r)
            merge(A, p, q, r)
        
        - To sort an entire array, we need to call mergeSort(A, 0, length(A)-1).
        - As shown in the image below, the merge sort algorithm recursively divides the array into havles until
          we reach the base case of array with 1 element. After that, the merge function picks up the sorted sub-arrays
          and merges them to gradually sort the entire array.
        
        The merge Step of Merge Sort:
            - Every recursive algorhtm is dependent on a base case and the ability to combine the results from base cases.
              Merge sort is no different. The most oimportant part of the merge sort algorithm is, you guessed it, merge step.
            - The merge step is the solution to the simple problem of merging two sorted lists(arrays) to build one large sorted list(array).
            - The algorhtm maintains three pointers, one for each of the two arrays and one for maintaining the current index of the final
              sorted array.
            
            Have we reached the end of any of the arrays?
                No:
                    Compare current elements of both arrays 
                    Copy smaller element into sorted array
                    Move pointer of element containing smaller element
                Yes:
                    Copy all remaining elements of non-empty array
        
        Writing the Code for Merge Algorithm:
            - A noticeable difference between the merging step we described above and the one we use for merge sort is that we only perform
              the merge function on consecutive sub-arrays.
            - This is why we only need the array, the first position, the last index of the first subarray(we can calculate the first index of the second subarray)
              and the last index of the second subarray.
            - Our task is to merge two subarrays A[p..q] and A[q+1..r] to create a sorted array A[p..r]. So the inputs to the function are A,p,q, and r
            The merge function works as follows:
                1. Create copies of the subarrays L <- A[p..q] and M <- A[q+1..r].
                2. Create three pointers i, j, and k
                    a. i maintains current index of L, starting at 1
                    b. j maintains current index of M, starting at 1
                    c. k maintains the current index of A[p..q], starting at p.
                3. Until we reach the end of either L or M, pick the larger among the elements from L and M and place them in the correct position at A[p..q]
                4. WHen we run out of elements in either L or M, pick up the remaining elements and put in A[p..q].
            
            In code:
                // Merge two subarrays L and M into arr
                void merge(int arr[], int p, int q, int r) {

                    // Create L ← A[p..q] and M ← A[q+1..r]
                    int n1 = q - p + 1;
                    int n2 = r - q;

                    int L[n1], M[n2];

                    for (int i = 0; i < n1; i++)
                        L[i] = arr[p + i];
                    for (int j = 0; j < n2; j++)
                        M[j] = arr[q + 1 + j];

                    // Maintain current index of sub-arrays and main array
                    int i, j, k;
                    i = 0;
                    j = 0;
                    k = p;

                    // Until we reach either end of either L or M, pick larger among
                    // elements L and M and place them in the correct position at A[p..r]
                    while (i < n1 && j < n2) {
                        if (L[i] <= M[j]) {
                            arr[k] = L[i];
                            i++;
                        } else {
                            arr[k] = M[j];
                            j++;
                        }
                        k++;
                    }

                    // When we run out of elements in either L or M,
                    // pick up the remaining elements and put in A[p..r]
                    while (i < n1) {
                        arr[k] = L[i];
                        i++;
                        k++;
                    }

                    while (j < n2) {
                        arr[k] = M[j];
                        j++;
                        k++;
                    }
                }
        
        Merge() Function Explained Step-By-Step
            - A lot is happening in this function, so let's take an example to see how this would work.
            - The array A[0..5] contains two sorted subarrays A[0..3] and A[4..5]. Let us see how the merge
              function will merge the two arrays.
                void merge(int arr[], int p, int q, int r) {
                // Here, p = 0, q = 4, r = 6 (size of array)
            Step 1: Create duplicate copies of sub-arrays to be sorted:
                // Create L ← A[p..q] and M ← A[q+1..r]
                int n1 = q - p + 1 = 3 - 0 + 1 = 4;
                int n2 = r - q = 5 - 3 = 2;

                int L[4], M[2];

                for (int i = 0; i < 4; i++)
                    L[i] = arr[p + i];
                    // L[0,1,2,3] = A[0,1,2,3] = [1,5,10,12]

                for (int j = 0; j < 2; j++)
                    M[j] = arr[q + 1 + j];
                    // M[0,1] = A[4,5] = [6,9]
            Step 2: Maintain current index of sub-arrays and main array
                int i, j, k;
                i = 0; 
                j = 0; 
                k = p;
            Step 3: Until we reach the end of either L or M, pick larger among elements L and M and place
                    them in the correct position at A[p..r]
                while (i < n1 && j < n2) { 
                    if (L[i] <= M[j]) { 
                        arr[k] = L[i]; i++; 
                    } 
                    else { 
                        arr[k] = M[j]; 
                        j++; 
                    } 
                    k++; 
                }
            Step 4: When we run out of elements in either L or M, pick up the remaining elements
                    and put in A[p..r]
                // We exited the earlier loop because j < n2 doesn't hold
                while (i < n1)
                {
                    arr[k] = L[i];
                    i++;
                    k++;
                }
                // We exited the earlier loop because i < n1 doesn't hold  
                while (j < n2)
                {
                    arr[k] = M[j];
                    j++;
                    k++;
                }
            }
                - This step would have been needed if the size of M was greater than L.
                - At the end of the merge function, the subarray A[p..r] is sorted.

    Merge Sort Complexity:
        Time Complexity:
            Best: O(n*log n)
            Worst: O(n*log n)
            Average: O(n*log n)
        Space Complexity: O(n)
        Stability: Yes
    
    Merge Sort Applications:
        - Inversion count problem
        - External sorting
        - E-commerce applications
    
    Similar Sorting Algorithms:
        - Quicksort
        - Insertion sort
        - Selection sort
        - Bucket sort
    
=============================================================================================

Quicksort Algorithm:
    - Quicksort is a sorting algorithm based on the divide and conquer approach where
        1. An array is divided into subarrays by selecting a pivot element (element selected from the array).

           While dividing the array, the pivot element should be positioned in such a way that elements
           less than pivot are kept on the left side and elements are greater than pivot are on the
           right side of the pivot.
        
        2. The left and right subarrays are also divided using the same approach. This process
           continues until each subarray contains a single element.
        
        3. At this point, elements are already sorted. Finally, elements are combined to form a sorted array.
    
    Working of Quicksort Algorith:
        1. Select the Pivot element:
            - There are different variations of quicksort where the pivot element is selected from different positions.
              Here, we will be selecting the rightmost element of the array as the pivot element.
        2. Rearrange the array:
            - Now the elements of the array are rearranged so that elements that are smaller than the pivot are put
              on the left and the elements greater than the pivot are put on the right.

              Here's how we rearrange the array:
                1. A pointer is fixed at the pivot element. The pivot element is compared with the elements
                   beginning from the first index.
                2. If the element is greater than the pivot element, a second pointer is set for that element.
                3. Now, pivot is compared with other elements. If an element smaller than the pivot element is reached,
                   the smaller element is swapped with the greater element found earlier.
                4. Again, the process is repeated to set the next greater element as the second pointer. And, swap it with
                   another smaller element.
                5. The process goes on until the second last element is reached.
                6. Finally, the pivot element is swapped with the second pointer.
        3. Divide Subarrays:
            - Pivot elements are again chosen for the left and the right sub-parts separately. And, step 2 is repeated.
            - The subarrays are divided until each subarray is formed of a single element. At this point, the array is already sorted.
    
    Quicksort Algorithm:
        quickSort(array, leftmostIndex, rightmostIndex)
            if (leftmostIndex < rightmostIndex)
                pivotIndex <- partition(array,leftmostIndex, rightmostIndex)
                quickSort(array, leftmostIndex, pivotIndex - 1)
                quickSort(array, pivotIndex, rightmostIndex)

        partition(array, leftmostIndex, rightmostIndex)
            set rightmostIndex as pivotIndex
            storeIndex <- leftmostIndex - 1
            for i <- leftmostIndex + 1 to rightmostIndex
                if element[i] < pivotElement
                    swap element[i] and element[storeIndex]
                    storeIndex++
            swap pivotElement and element[storeIndex+1]
            return storeIndex + 1
    
    Quicksort Complexity:
        Time Complexity:
            Best: O(n*log n)
            Worst: O(n^2)
            Average: O(n*log n)
        Space Complexity: O(log n)
        Stability: No
    
    Time Complexities:
        Worst Case Complexity [Big-O]: O(n^2):
            - It occurs when the pivot element picked is either the greatst or the smallest element.
            - This condition leads to the case in which the pivot element lies in an extreme end of the sorted array.
              One sub-array os always empty and another sub-array contains n-1 element. Thus, quicksort
              is called only on this sub-array.
            - However, the quicksort algorithm has better performance for scattered pivots.
        Best Case Complexity [Big-Omega]: O(n*log n):
            - It occurs when the pivot elemnt is always the middle element or near to the middle element.
        Average Case Complexity [Big-Theta]: O(n*log n):
            - It occurs when the above conditions do not occur.
    
    Space Complexity:
        - The space complexity for quicksort is O(log n)
    
    Quicksort Applications:
        - The programming language is good for recursion
        - Time complexity matters
        - Space complexity matters
    
    Simiar Sorting Algorithms:
        - Insertion Sort
        - Merge Sort
        - Selection Sort
        - Bucket Sort

=============================================================================================================================

Counting Sort Algorithm:
    - Counting sort is a sorting algorithm that sorts the elements of an array by counting the number of occurrences of each
      unique element in the array.
    - The count is stored in an auxiliary array and the sorting is done by mapping the count as an index of the auxiliary array.

    Working of Counting Sort:
        1. Find out hte maximum element (let it be max) from the given array.
        2. Initialize an array of length max+1 with all elements 0.
           This array is used for storing the count of the elements in the array.
        3. Store the count of each element at their respective index in count array
           For example: if the count of element 3 is 2 then, 2 is stored in the 3rd position of count array. If element "5"
           is not present in the array, then 0 is stored in 5th position.
        4. Store cumulative sum of the elements of the count array. It helps in placing the elements into the correct index
           of the stored array.
        5. Find the index of each element of the original array in the count array. This gives the cumulative count.
           Place the element at the index calculated as shown in figure below.
        6. After placing each element at its correct position, decrease its count by one.
    
    Counting Sort Algorithm:
        countingSort(array, size)
            max <- find largest element in array
            initialize count array with all zeros
            for j <- 0 to size
                find the total count of each unique element and 
                store the count at jth index in count array
            for i <- 1 to max
                find the cumulative sum and store it in count array itself
            for j <- size down to 1
                restore the elements to array
                decrease count of each element restored by 1
    
    Counting Sort Code: exmaple files

    Complexity:
        Time Complexity:
            Best: O(n+k)
            Worst: O(n+k)
            Average: O(n+k)
        Space Complexity: O(max)
        Stability: Yes
    
    Time Complexities:
        There are mainly four main loops. (Finding the greatest
        value can be done outside the function)
        for-loop    time of counting
        1st         O(max)
        2nd         O(size)
        3rd         O(max)
        4th         O(size)
        - Overall copmlexity = O(max)+O(size)+O(max)+O(size) = O(max+size)
        - Worst Case Complexity: O(n+k)
        - Best Case Complexity: O(n+k)
        - Average Case Complexity: O(n+k)
        - In all the above cases, the complexity is the same because 
          no matter how the elements are placed in the array, the algorithm goes through n+k times.
        - There is no comparison between any elements, so it is bad if the integers are very large
          because the array of that size should be made.
    
    Space Complexity:
        - The space complexity of Counting Sort is O(max). Larger the range of elements, larger the space complexity.

    Counting Sort Applications:
        - there are smaller integers with multiple counts.
        - linear complexity is the need.
    
    Similar Sorting Algorthims:
        - Quicksort
        - Merge Sort
        - Bucket Sort
        - Radix Sort

=======================================================================================================================

Radix Sort Algorithm:
    - Radix sort is a sorting algorithm that sorts the elements by first grouping the individual digits of the same place value.
    - Then, sort the elements according to their increasing/decreasing order.
    - Suppose, we have an array of 8 elements. First, we will sort elements based on the value of the unit place.
    - Then, we will sort elements based on the value of the tength place.
    - This process goes on until the last significant place.
    - Let the initial array be [121, 432, 562, 23, 1, 45, 788]. It is sorted according to radix sort.

    Working of Radix Sort:
        1. Find the largest element in the array, ie. max. Let X be the number of digits in max. X is calculated because
           we have to go through all the significant places of all elements.
           In this array [121, 432, 564, 23, 1, 45, 788], we have the largest number 788. It has 3 digits. Therefore,
           the loop should go up to hundreds place (3 times).
        2. Now go through each signficiant place one by one.
           Use any stable sorting technique to sort the digits at each significant place. We have used counting sort.
           Sort the elements based on the unit place digits (X=0).
        3. Now, sort the elements based on digits at tense place.
        4. Finally, sort the elements based on the digits at hundreds place.
    
    Radix Sort Algorithm:
        radixSort(array)
            d <- maximum number of digits in the largest element
            create d buckets of size 0-9
            for i <- 0 to d
                sort the elements according to ith place digits using countingSort

        countingSort(array, d)
            max <- find largest element among dth place elements
            initialize count array with all zeros
            for j <- 0 to size
                find the total count of each unique digit in dth place of elements and
                store the count at jth index in count array
            for i <- 1 to max
                find the cumulative sum and store it in count array itself
            for j <- size down to 1
                restore the elements to array
                decrease count of each element restored by 1
        
    Radix Sort Code: example files

    Radix Sort Complexity:
        Time Complexity:
            Best: O(n+k)
            Worst: O(n+k)
            Average: O(n+k)
        Space Complexity: O(max)
        Stability: Yes

        - Since radix sort is a non-comparative algorithm, it has advantages over comparative sorting algorithms.
        - For the radix sort that uses counting sort as an intermediate stable sort, the time complexity is O(d(n+k)).
        - Here, d is the number cycle and O(n+k) is the time complexity of counting sort.
        - Thus, radix sort has linear time complexity which is better than O(nlog n) of comparative sorting algorithms.
        - If we take very large digit numbers or the number of other bases like 32-bit and 64-bit numbers then it can perform
            linear time howeever, the intermediate sort takes large space.
        - This makes radix sort space inefficient. This is the reason why this sort is not used in software libraries.
    
    Radix Sort Applications:
        - DC3 algorithm (Karkkainen-Sanders-Burkhardt) while making a suffix array.
        - places where there are numbers in large ranges.
    
    Similar Sorting Algorithms:
        - Quicksort
        - Merge Sort
        - Bucket Sort
        - Counting Sort
    
============================================================================================================================================

Bucket Sort:
    - Bucket Sort is a sorting algorithm that divides the unsorted array elements into several groups called buckets.
    - Each bucket is then sorted by using any of the suitable sorting algorithms or recursively applying the same bucket algorithm.
    - Finally, the sorted buckets are combined to form a final sorted array.

    Scatter Gather Approach:
        - The process of bucket sort can be understood as a scatter-gather approach.
        - Here, elements are first scattered into buckets then the elemnets in each bucket are sorted.
        - Finally, the elements are gathered in order.
    
    Working of Bucket Sort:
        1. Suppose, the input array is:
            0.42, 0.32, 0.23, 0.52, 0.25, 0.47, 0.51
           Create an array of size 10. Each slot of this array is used as a bucket for storing elements.
        2. Insert elements into the buckets from the array. The elements are inserted according to the range of the bucket.
           In our example code, we have buckets each of ranges from 0 to 1, 1 to 2, 2 to 3..... (n-1) to n.
           Suppose, an input element is 0.23 is taken. It is multiplied by size = 10 (ie. 0.23*10=2.3)
           Then it is converted into an integer (ie. 2.3=2). Finally, 0.23 is inserted into bucket-2.
           
           Similarly, 0.23 is also inserted into the same bucket.
           Everytime, the floor value of the floating point number is taken.

           If we take integer numbers as input, we have to divide it by the interval (10 here) to get the floor value.
           
           Similarly, other elements are inserted into their respective buckets.
        3. The elemnts of each bucket are sorted using any of the stable sorting algorithms. Here, we have used quicksort (inbuilt function)
        4. The elements from each bucket are gathered.
           It is done by iterating through the bucket and inserting an individual element into the original array in each cycle.
           The element from the bucket is erased once it is copied into the original array. 
        
    Bucket Sort Algorithm:
        bucketSort()
            create N buckets each of which can hold a range of values
            for all the buckets
                initialize each bucket with 0 values
            for all the buckets
                put elements into buckets matching the range
            for all the buckets 
                sort elements in each bucket
            gather elements from each bucket
        end bucketSort
    
    Bucket Sort Complexity:
        Time Complexity:
            Best: O(n+k)
            Worst: O(n^2)
            Average: O(n)
        Space Complexity: O(n+k)
        Stability: Yes

        Worst Case Complexity: O(n^2)
            - When there are elements of close range in the array, they are likely to be placed in the same bucket.
            - This may result in some buckets having more number of elements than others.
            - It makes the copmlexity depend on the sorting algorithm used to sort the elements of the bucket.
            - The complexity becomes even worse when the elements are in reverse order.
            - If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n^2).
        
        Best Case Complexity: O(n+k)
            - It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
            - The complexity becomes even better if the elements inside the buckets are already sorted.
            - If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k).
            - O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms
              having linear time complexity at best case.
        
        Average Case Complexity: O(n)
            - It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time.
            - It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements.
        
    Bucket Sort Applications:
        - Input is uniformly distributed over a range.
        - There are floating point values
    
    Similar Sorting Algorithms:
        - Bubble Sort
        - Quicksort
        - Insertion Sort
        - Merge Sort
        - Selection Sort 

=========================================================================================

Heap Sort Algorithm:
    - Heap sort is a popular and efficient sorting algorithm in computer programming.
    - Learning how to write the heap sort algorithm requires knowledge of two types of data structures
      arrays and trees.
    - The inital set of numbers that we want to sort is stored in an array ex. [10, 3, 76, 34, 23, 32]
      and after sorting, we get a sorted array [3, 10, 23, 32, 34, 76].
    - Heap sort works by visualizing the elements of the array as a special kind of complete binary tree
      called a heap.
    
    Relationship between Array Indexes and Tree Elements:
        - A complete binary tree has an interesting property that we can use to find the children and parents of any node.
        - If the index of any element in the array is i, the element in the index 2i+1 will become the left child and
          element in 2i+2 index will become the right child.
        - Also, the parent of any element at index i is given by the lower bound of (i-1)/2.
    
    Heap Data Structure:
        - Heap is a special tree-based data structure. A binary tree is said to follow a heap data structure if
            - it is a complete binary tree
            - All nodes in the tree follow the property that they are greater than their children ie. the largest element
              is at the root and both its children and smaller than the root and so on.
            - Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap.
        How to "heapify" a tree:
            - Starting from a complete binary tree, we can modify it to become a Max-Heap by running a function called heapify
              on all the non-leaf elements of the heap.
            - Since heapify uses recursion, it can be difficult to grasp.
            - So let's first think about how you would heapify a tree with just three elements.
            heapify(array)
                Root = array[0]
                Largest = largest( array[0] , array [2*0 + 1]. array[2*0+2])
                if(Root != Largest)
                    Swap(Root, Largest)
            - The example above shows two scenarios - one in which the root is the largest element and we don't need to do anything.
            - And another in which the root had a larger element as a child and we needed to swap to maintain max-heap property.
            - If you've worked with recursive algorithms before, you've probably identified that this must be the base case.
            - Now let's think of another scenario in which there is more than one level.
            - The top element isn't a max-heap but all the sub-trees are max-heaps.
            - To maintain the max-heap property for the entire tree, we will have to keep pushing 2 downwards until it reaches its correct position.
            - Thus, to maintain the max-heap property in a tree where both sub-trees are max-heaps, we need to run heapify on the root element repeatedly
              until it is larger than its children or it becomes a leaf node.
            - We can combine both these conditions in one heapify function as
            void heapify(int arr[], int n, int i) {
                // Find largest among root, left child and right child
                int largest = i;
                int left = 2 * i + 1;
                int right = 2 * i + 2;

                if (left < n && arr[left] > arr[largest])
                    largest = left;

                if (right < n && arr[right] > arr[largest])
                    largest = right;

                // Swap and continue heapifying if root is not largest
                if (largest != i) {
                    swap(&arr[i], &arr[largest]);
                    heapify(arr, n, largest);
                }
            }
            - This function works for both the base case and for a tree of anu size.
            - We can thus move the root element to the correct position to maintain the max-heap status for any tree size as long as the sub-trees are max-heaps.
        
    Build max-heap:
        - To build a max-heap from any tree, we can thus start heapifying each sub-tree from the bottom up
            and end up wit ha max-heap after the function is applied to all the elements including the root element.
        - In the case of a complete tree, the first index of a non-leaf node is given by n/2-1. All other nodes after that are leaf-nodes and thus don't need to be heapified.
        - So we can build a maximum heap as:
            // Build heap (rearrange array)
            for (int i = n / 2 - 1; i >= 0; i--)
            heapify(arr, n, i);
        - As shown in the above diagram, we start by heapifying the lowest smallest trees and gradually move up
            until we reach the root element.
        - If you've understood everything till here, congratulations, you are on your way to mastering the Heap sort.
    
    Working of Heap Sort:
        1. Since the tree satisfies Max-Heap property, then the largest item is stored at the root node.
        2. Swap: Remove the element and put at the end of the array (nth position) Put the last item of the tree (heap) at the vacant place.
        3. Remove: Reduce the size of the heap by 1.
        4. Heapify: Heapify the root element again so that we have the highest element at root.
        5. The process is repeated until all the items of the list are sorted.
        // Heap sort
        for (int i = n - 1; i >= 0; i--) {
        swap(&arr[0], &arr[i]);

        // Heapify root element to get highest element at root again
        heapify(arr, i, 0);
        }
    
    Heap Sort Code: example files

    Heap Sort Complexity:
        Time Complexity:
            Best: O(nlog n)
            Worst: O(nlog n)
            Average: O(nlog n)
        Space Complexity: O(1)
        Stability: No

        - Heap sort has O(nlog n) time complexities for all the cases (best case, average case, worst case)
        - Let us understand the reason why. The height of a complete binary tree containing n elements is log n 
        - As we have seen earlier, to fully heapify an element whose subtrees are already max-heaps,
            we need to keep comparing the element with its left and right children and pushing it downwards
            until it reaches a point where both its children are smaller than it.
        - In the worst case scenario, we will need to move an element from the root to the leaf node making a multiple
            of log(n) comparisons and swaps.
        - During the build-max-heap stage, we do that for n/2 elements so the worst case complexity of the build_heap step
            is n/2*log n ~ nlog n.
        - During the sorting step, we exchange the root element with the last element and heapify the root element.
            For each element, this again takes log n worst time because we might have to bring the element all the way from the
            root to the leaf. Since we repeat this n times, the heap_sort step is also nlog n.
        - Also since the build_max_heap and heap_sort steps are executed on after another, the algorithmic complexity is not
            multiplied and it remains in the order of nlog n.
        - Also it performs sorting in O(1) space complexity.
        - Compared with quick sort, it has a better worst case (O(nlog n)).
        - Quick sort has complexity O(n^2) for worst case. But in other cases, quicksort is fast.
        - Introsort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both:
            worst case speed of heapsort and average speed of quicksort.
    
    Heap Sort Applications:
        - Systems concerned with security and embedded systems such as Linux Kernel use Heap Sort because of the 
          O(nlog n) upper bound on Heapsort's running time and constant O(1) upper bound on its auxiliary storage.
        - Although Heap Sort has O(nlog n) time complexity even for the worst case, it doesn't have more applications
          (compared to other sorting algorithms like Quick Sort, Merge Sort).
        - However, its underlying data structure, heap, can be efficiently used if we want to extract the smallest (or largest)
          from the list of items without the overhead of keeping the remaining items in the sorted order. For ex. Priority Queues.
    
    Similar Sorting Algorithms:
        1. Quicksort
        2. Merge Sort

==============================================================================================================================================

