Dynamic Programming:
    - Dynamic Programming is a technique in computer programming that helps
      to efficiently solve a class of problems that have overlapping subproblems
      and optimal substructure property.
    - If any problem can be divided into subproblems, which in turn are divided
      into smaller subprobelms, and if there are overlapping among these subproblems,
      then the solutions to these subproblems can be saved for future reference.
      In this way, efficiency of the CPU can be enhanced. This method of solving a
      solution is referred to as dynamic programming.
    - Such problems involve repeatedly calculating the valuye of the same subproblems
      to find the optimal solution.
    
    Dynamic Programming Example:
        - Let's find the fibonacci sequence upto 5th term. A fibonacci series is the
          sequence of numbers in which each number is the sum of the two preceding ones.
          For example, 0, 1, 1, 2, 3. Here, each number is the sum of the two preceding numbers.
        
        Algorithm:
            Let n be the number of terms.

            1. If n <= 1, return 1.
            2. Else, return the sum of two preceding numbers.
        
            We are calculating the fibonacci sequence up to the 5th term.
                1. The first term is 0.
                2. The second term is 1.
                3. The third term is sum of 0 (from step 1) and 1 (from step 2), which is 1.
                4. The fourth term is the sum of the third term (from step 2), ie. 1 + 1 = 2.
                5. The fifth term is the sum of the fourth term (from step 4) and third term (from step 3)
                   ie. 2 + 1 = 3
            
            Hence, we have the sequence 0, 1, 1, 2, 3. Here, we have used the results of the previous steps
            as shown below. This is called a dynamic programming approach.
            F(0) = 0
            F(1) = 1
            F(2) = F(1) + F(0)
            F(3) = F(2) + F(1)
            F(4) = F(3) + F(2)

    How Dynamic Programming Works:
        - Dynamic programming works by storing the result of subproblems so that when their solutions are required,
          they are at hand and we do not need to recalculate them.
        - This technique of storing the value of subproblems is called memoization. By saving the values in the
          array, we save time for computations of sub-problems we have already come across.
        var m = map(0 → 0, 1 → 1)
        function fib(n)
            if key n is not in map m 
                m[n] = fib(n − 1) + fib(n − 2)
            return m[n]
        - Dynamic programming by memoization is a top-down approach to dynamic programming. By reversing the
          direction in which the algorithm works ie. by starting from the base case and working towards the solution,
          we can also implement dynamic programning in a bottom-up manner.
        function fib(n)
            if n = 0
                return 0
            else
                var prevFib = 0, currFib = 1
                repeat n − 1 times
                    var newFib = prevFib + currFib
                    prevFib = currFib
                    currFib  = newFib
            return currentFib
    
    Recursion vs Dynamic Programming:
        - Dynamic programming is mostly applied to recursive algorithms. This is not a coincidence, most optimization
          problems require recursion and dynamic programming is used for optimization.
        - But not all problems that use recursion can use Dynamic programming. Unless there is a presence of overlapping
          subproblems like in the fibonacci sequence problems, a recursion can only reach the solution using a divide and conquer approach.
        - That is the reason why a recursive algorithm like Merge Sort cannot use Dynamic Programming, because the subproblems are
          not overlapping in any way.
    
    Greedy Algorithm vs Dynamic Programming:
        - Greedy Algorithms are similar to dunamic programming in the sense that they are both tools for optimization.
        - However, greddy algorithms look for locally optimum solutions or in other words, a greedy chocie, in the hopes of finding a global optimum.
        - Dynamic programming, on the other hand, finds the optimal solution to subproblems and then makes an informed choice to combine the results
          of those subproblems to find the most optimum solution.

    Different Types of Dynamic Programming Algorithms:
        - Longest Common Subsequence
        - Floyd-Warshall Algorithm

======================================================================================================================================================

Floyd-Warshall Algorithm:
	- Floyd-Warshall Algorithm is an algorithm for finding the shortest oath between all the pairs of vertices in a weighted graph.
	  This algorithm works for both the directed and undirected weighted graphs. Butm it does not work for the graphs with negative cycles
	  (where the sum of the edges in a cycle is negative).
	- Floyd-Warshall algorithm is also called as Floyd's algorithm, Roy-Floyd algorithm, Roy-Warshall algorithm, or WFI algorithm.
	- This algorithm follows the dynamic programming approach to find the shortes paths.

	How Floyd-Warshall Works?
		1. Create a matrix A^0 of dimension n*n where n is the number of vertices. The row and the column are indexed as i and j respectively.
		   i and j are the vertices of the graph.

		   Each cell A[i][j] is filled with the distance from the ith vertex to the jth vertex. If there is no path from ith vertex to jth vertex,
		   the cell is left as infinity.
		
		2. Now, create a matrix A^1 using matrix A^0. The elements in the first column and the first row are left as they are.
		   The remaining cells are filled in the following way.

		   Let k be the intermediate vertex in the shortes path from source to destination. In this step, k is the first vertex. A[i][j] is filled with
		   (A[i][k] + A[k][j]) if (A[i][j] > A[i][k] + A[k][j]).

		   That is, if the direct distance from the source to the destination is greater than the path through the vertex k, then the cell is filled with
		   A[i][k] + A[k][j].

		   In this step, k is vertex 1. We calculate the distance from source vertex to destination vertex through this vertex k.

		   For example: For A^1[2, 4], the firect distance from vertex 2 to 4 is 4 and the sum of the distance from vertex 2 to 4 through vertex (ie. from vertex 2 to 1
		   and from vertex 1 to 4) is 7. Since 4 < 7, A^0[2, 4] is filled with 4.
		
		3. Similarly, A^2 is created using A^1. The elements in the second column and the second row are left as they are.

		   In this step, k is the second vertex (ie. vertex 2). The remaining steps are the same as in step 2.
		
		4. Similarly, A^3 and A^4 is also created.

		5. A^4 gives the shortes path between each pair of vertices.
	
	Floyd-Warshall Algorithm:
		n = no of vertices
		A = matrix of dimension n*n
		for k = 1 to n
			for i = 1 to n
				for j = 1 to n
					Ak[i, j] = min (Ak-1[i, j], Ak-1[i, k] + Ak-1[k, j])
		return A
	
	Floyd-Warshall Example: Example Files

	Floyd Warshall Algorithm Complexity:
		Time Complexity:
			- There are three loops. Each loop has constant complexities. So the time complexity of the Floyd-Warshall algorithm i O(n^3).
		Space Complexirt:
			- The spacec complexity of the Floyd-Warshall algorithm is O(n^2).
	
	Floyd Warshall Algorithm Applications
		- To find the shortes path is a directed graph
		- To find the transitive closure of directed graphs
		- To find the Inversion of real matrices
		- For testing whether an unfirected graph is bipartite.

===========================================================================================================================================================================

Longest Common Subsequence:
	- The longest common subsequence (LCS) is defined as the longest subsequence that is common to all the given sequences, provided
	  that the elements of the subsequence are not required to occupy consecutive positions within the original sequences.
	- If S1 and S2 are the two given sequences then, Z is the common subsequence of S1 and S2 if Z is a subsequence of both S1 and S2. Furthermore,
	  Z must be a strictly increasing sequence of the indices of both S1 and S2.
	- In a strictly increasing sequence, the indices of the elements chosen from the original sequences must be in ascending order in Z.
	- If S1 = {B, C, D, A, A, C, D} Then, {A, D, B} cannot be a subsequence of S1 as the order of the elements is not the same (ie. not strictly increasing sequence)

	LCS Example:
		S1 = {B, C, D, A, A, C, D}
		S2 = {A, C, D, B, A, C}
		Common subseuqneces are {B, C}, {C, D, A, C}, {A, A, C}, {A, C}, {C, D}, ...
		Among these subsequences, {C, D, A, C} is the longest common subsequence.
		- We are going to find the longest common subsequence using dynamic programming.
	
	Using Dynamic Programming to find the LCS:
		Let us take two sequences:
			X = {A, C, A, D, B}
			Y = {C, B, D, A}
		The following steps are followed for finding the longest common subsequence.
		1. Create a table of dimension n+1*m+1 where n and m are the lengths of X and Y respectively.
		   The first row and the first column are filled with zeros.
		2. Fill each cell of the table using the following logic.
		3. If the character corresponding to the current row and current column are matching, then fill the current cell by adding one to the diagonal element.
		   Point an arrow to the diagonal cell.
		4. Else take the maximum value from the previous column and previous row element for filling the current cell.
		   Point an arrow to the cell with maximum value. If they are equal, point to any of them.
		5. Step 2 is repeated until the table is filled.
		6. The value in the last row and the last column is the length of the longest common subsequence.
		7. In order to find the longest common subsequence, start from the last element and follow the direction of the arrow.
		   The elements are corresponding to () symbol form the longest common subsequence.
		Thus the longest common subsequence is CA.

	How is a dynamic programming algorithm more efficient than the recursive algorithm while solving an LCS problem?
		- The method of dynamic programming reduces the number of function calls. It stores the result of each function call so that is can be used in future
		  calls without the need for redundant calls.
		- In the above dynamic algorithm, the results obtained from each comparison between elements of X and the elements of Y are stored in a table
		  so that they can be used in future comparisons.
		- So, the time taken by a dynamic approach is the time taken to fill the table O(mn).
		  Whereas, the recursion algorithm has the complexity of 2^max(m, n).
	
	Longest Common Subsequence Algorithm:
		X and Y be two given sequences
		Initialize a table LCS of dimension X.length * Y.length
		X.label = X
		Y.label = Y
		LCS[0][] = 0
		LCS[][0] = 0
		Start from LCS[1][1]
		Compare X[i] and Y[j]
			If X[i] = Y[j]
				LCS[i][j] = 1 + LCS[i-1, j-1]   
				Point an arrow to LCS[i][j]
			Else
				LCS[i][j] = max(LCS[i-1][j], LCS[i][j-1])
				Point an arrow to max(LCS[i-1][j], LCS[i][j-1])
	
	Longest Common Subsequence Code: example Files

	Longest Common Subsequence Applications:
		1. In compressin genome resequencing data
		2. To authenticate users within their mobile phone through in-air signatures.

=====================================================================================================================================================================

Backtracking Algorithm:
	- A backtracking algorithm is a problem-solving algorithm that uses a brute force approach for finding the desired output.
	- The Brute force approach tries out all the possible solitions and chooses the desired/best solutions.
	- The term backtracking suggests that if the current solution is not suitable, then backtrack and try other solutions.
	  Thus, recursion is used in this approach.
	- This approach is used to solve problems that have multuple solutions. If you want an optimal solution, you must go for dynamic programming.
	State Space Tree:
		- A space state tree is a tree representing all the possible states (solution or nonsolution) of the problem from the root as an initial state to the leaf
		  as a terminal state.
	
	Backtracking Algorithm:
		Backtrack(x)
			if x is not a solution
				return false
			if x is a new solution
				add to list of solutions
			backtrack(expand x)
	
	Example of Backtracking Approach:
		- Problem: You want to find all the possible ways of arranging 2 boys and 1 girl on 3 benchs.
		  Constraint: Girl should not be on the middle bench.
		- Solution: There are a total of 3! = 6 possibilities. We will try all the possibilities and get the possible
		  solutions. We recursively try all the possibilities.
		- All the possibilities are:
			B1B2G
			B1GB2
			B2B1G
			B2GB1
			GB1B2
			GB2B1
		- Find the following state space tree in the solutions on website.
	
	Backtracking Algorithm Applications:
		1. To find all Hamiltonian Paths present in a graph.
		2. To solve the N Queen problem.
		3. Maze solving problem.
		4. The Knight's tour problem.

===================================================================================================================================

Rabin-Karp Algorithm:
	- Rabin-Karp algorithm is an algorithm used for searching/matching patterns in the text using a hash function.
	- Unlike Naive string matching algorithm, it does not travel through every character in the inital phase, rather it filters
	  the characters that do not match and then performs the comparison.
	- A hash function is a tool to map a larger input value to a smaller output value. This output value is called the hash value.

	How does Rabin-Karp Algorithm Work?:
		- A sequence of characters is taken and checked for the possibility of the presence of the required string.
		- If the possibilty is found then, character matching is performed.
		Let us understand the algorithm with the following steps.
			1. Let the text be:
				ABCCDDAEFG
			   And the string to be searched in the above text be:
				CDD
			2. Let us assign a numerical value(v)/weight for the characters we will be using in the problem. Here, we have taken first ten 
			   alphabets only (ie. A to J).
			   	ABCDEFGHIJ
				12345678910
			3. n be the length of the pattern and m be the length of the text. Here, m = 10 and n = 3.
			   Let d be the number of characters in the input set.
			   Here, we have ten input set {A, B, C, ..., J}. So, d = 10.
			   You can assume any suitable value for d.
			4. Let us calculate the hash value of the pattern.
				CDD
				hash value = 6
			5. Calculate the hash value for the text-window of size m.
				hash value for pattern(p) = Σ(v * dm-1) mod 13 
                      = ((3 * 102) + (4 * 101) + (4 * 100)) mod 13 
                      = 344 mod 13 
                      = 6
			6. Compare the hash value of the pattern with the has vlaue of the text. If they match then, character-matching is performed.
			   In the above examples, the hash value of the first window (ie. t) matches with p so, go for character matching between
			   ABC and CDD. Since they do not match so, go for the next window.
			7. We calculate the hash value of the nexdt window by subtracting the first term and adding the next term as shown below.
			   t = ((1 * 102) + ((2 * 101) + (3 * 100)) * 10 + (3 * 100)) mod 13 
				 = 233 mod 13  
				 = 12
			   In order to optimize this process, we make use of the previous hash value in the following way.
			   t = ((d * (t - v[character to be removed] * h) + v[character to be added] ) mod 13  
				 = ((10 * (6 - 1 * 9) + 3 )mod 13  
				 = 12
				Where, h = dm-1 = 103-1 = 100.
			8. For BCC, t = 12 (not = 6). THerefore, go for the next window. After a few searches, we will get the match for the window
			   CDA in the text.
			   ABCCDDAEFG
			   ABC hash value = 6
			   CDD hash value = 6
			   Combined hash value = 12
		
		Algorithm:
			n = t.length
			m = p.length
			h = dm-1 mod q
			p = 0
			t0 = 0
			for i = 1 to m
				p = (dp + p[i]) mod q
				t0 = (dt0 + t[i]) mod q
			for s = 0 to n - m
				if p = ts
					if p[1.....m] = t[s + 1..... s + m]
						print "pattern found at position" s
				If s < n-m
					ts + 1 = (d (ts - t[s + 1]h) + t[s + m + 1]) mod q
		
		Example:
		# Rabin-Karp algorithm in python


		d = 10

		def search(pattern, text, q):
			m = len(pattern)
			n = len(text)
			p = 0
			t = 0
			h = 1
			i = 0
			j = 0

			for i in range(m-1):
				h = (h*d) % q

			# Calculate hash value for pattern and text
			for i in range(m):
				p = (d*p + ord(pattern[i])) % q
				t = (d*t + ord(text[i])) % q

			# Find the match
			for i in range(n-m+1):
				if p == t:
					for j in range(m):
						if text[i+j] != pattern[j]:
							break

					j += 1
					if j == m:
						print("Pattern is found at position: " + str(i+1))

				if i < n-m:
					t = (d*(t-ord(text[i])*h) + ord(text[i+m])) % q

					if t < 0:
						t = t+q


		text = "ABCCDDAEFG"
		pattern = "CDD"
		q = 13
		search(pattern, text, q)
	
	Limitations of Rabin-Karp Algorithm:
		Spurious Hit:
			WHen the hash value of the pattern matches with the hash value
			but the window is not the actual pattern then it is called a spurious hit.
			Spurious hit increases the time complexity of the algorithm. In order to minimize the spurious hit,
			we use modulus. It greatly reduces the spurious hit.
		
		Rabin-Karp Algorithm Complexity:
			- The average case and best case complexity of Rabin-Karp algorithm is O(m + n) and the worst case complexity is O(mn).
			- The worst-case complexity occurs when spurious hits occur a number for all the windows.
	
	Rabin-Karp Algorithm Applications:
		- For pattern matching
		- For searching string in a bigger text.